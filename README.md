# RHEO SNN - Brain & Survival Lab

A **Spiking Neural Network (SNN)** simulation featuring biologically-inspired learning with metabolic constraints and hormonal modulation.

## ğŸ§  Project Novelty

### Metabolic Constraints
Unlike traditional neural networks, RHEO neurons have:
- **Limited Energy**: Each neuron has a metabolic energy budget that depletes when firing
- **Fatigue**: Sustained activity leads to elevated firing thresholds
- **Recovery**: Energy regenerates over time, with layer-specific rates

### Hormonal Modulation
The network features neuromodulators that affect behavior:
- **Dopamine (DA)**: Reward signal for R-STDP learning
- **Acetylcholine (ACh)**: Attention/alertness when near obstacles
- **Serotonin (5HT)**: Stress response that spikes on failure
- **Exploration Noise**: Increases after failed epochs to try new paths

## ğŸ“ Project Structure

```
rheo-snn/
â”œâ”€â”€ main.py                    # Main application entry point
â”œâ”€â”€ brain_weights/             # Saved neural network weights
â”‚   â””â”€â”€ brain_weights.npz      # Trained brain (auto-generated)
â”œâ”€â”€ experiments/               # Simulation stats and logs
â”‚   â”œâ”€â”€ simulation_stats.json  # Latest experiment
â”‚   â””â”€â”€ simulation_stats_*.json # Timestamped backups
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ network.py             # Layer-based SNN with R-STDP
â”‚   â”œâ”€â”€ layer.py               # LIF neuron layer with metabolism
â”‚   â”œâ”€â”€ encoding.py            # Sensor-to-spike encoding
â”‚   â”œâ”€â”€ decoding.py            # Spike-to-motor decoding
â”‚   â”œâ”€â”€ monitor.py             # Performance tracking & analysis
â”‚   â”œâ”€â”€ environment/
â”‚   â”‚   â”œâ”€â”€ simulation.py      # Pygame environment with physics
â”‚   â”‚   â”œâ”€â”€ editor.py          # Visual map editor
â”‚   â”‚   â””â”€â”€ maps/              # Custom JSON map files
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ file_manager.py    # Directory & file utilities
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### Requirements
```bash
pip install numpy pygame
```

### Run the Simulation
```bash
python -m main
```

## ğŸ® Controls

### Main Menu
| Button | Function |
|--------|----------|
| **START** | Begin simulation on selected map |
| **SETTINGS** | Adjust sensors, range, hidden neurons |
| **LOAD** | Load previously trained brain |
| **ANALYSIS** | View detailed network statistics |
| **MAP EDITOR** | Create custom environments |
| **EXPERIMENTS** | Open experiment logs folder |

### During Simulation
| Key | Action |
|-----|--------|
| `+` / `-` | Increase/decrease simulation speed |
| `0` | Toggle Turbo Mode (10x speed) |
| `ESC` | Return to menu (auto-saves brain) |

## ğŸ—ºï¸ Map Editor

Create custom environments with the visual editor.

### Controls
| Key/Mouse | Action |
|-----------|--------|
| **Left Click** | Draw walls (drag to paint) |
| **Right Click** | Erase walls |
| **W** | Select Wall tool |
| **E** | Select Erase tool |
| **F** | Place Food spawn |
| **G** | Place Goal (gold target) |
| **S** | Set agent Spawn point |
| **Ctrl+S** | Save map |
| **Ctrl+L** | Load map |
| **ESC** | Exit editor |

Maps are saved to `src/environment/maps/` as JSON files.

## ğŸ’¾ Brain Persistence

### Auto-Save
The brain automatically saves to `brain_weights/brain_weights.npz` when:
- Returning to menu (ESC key)
- Completing an epoch with a successful goal

### Manual Load
Click **LOAD** in the menu to restore a previously trained brain. The network dimensions (sensors, hidden neurons) must match.

## ğŸ“Š Experiment Logs

Performance data is automatically saved to `experiments/`:
- **simulation_stats.json**: Latest session (overwritten)
- **simulation_stats_YYYYMMDD_HHMMSS.json**: Timestamped backups

### Logged Metrics
| Metric | Description |
|--------|-------------|
| `firing_rates` | Neural activity per layer over time |
| `energy_levels` | Metabolic health per layer |
| `rewards` | Reward signal history |
| `weights_mean/std` | Synaptic weight evolution |
| `success_epochs` | Goal completion times (learning curve) |

## ğŸ”¬ Network Architecture

```
Input Layer (Sensors)      Hidden Layer (Processing)     Output Layer (Motors)
     10 neurons       â†’         50 neurons          â†’        2 neurons
   High recovery            Recurrent connections          High energy cost
   Low energy cost          Moderate metabolism            Learnable via R-STDP
```

### Weight Matrices
| Connection | Shape | Learnable |
|------------|-------|-----------|
| Input â†’ Hidden | (50, 10) | No |
| Hidden â†’ Hidden | (50, 50) | No |
| Hidden â†’ Output | (2, 50) | **Yes (R-STDP)** |

## ğŸ“ˆ Learning Mechanism

### R-STDP (Reward-modulated Spike-Timing Dependent Plasticity)
1. **Eligibility Trace**: Tracks recent pre-post spike correlations
2. **Reward Signal**: Dopamine from goal (+500) or penalty (-50)
3. **Weight Update**: `Î”w = learning_rate Ã— eligibility Ã— dopamine`

### Exploration
After epoch timeout:
- Serotonin spikes (stress)
- Eligibility trace weakens recent paths
- Exploration noise increases 3x
- Forces agent to try new strategies

## ğŸ› ï¸ Configuration

Edit `main.py` to adjust defaults:
```python
DEFAULT_CONFIG = {
    'num_sensors': 10,    # Ray sensors
    'sensor_range': 180,  # Sensor distance (px)
    'num_hidden': 50,     # Hidden layer neurons
}
```

---

**Made with ğŸ§  and ğŸ’»**